{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flashbax as fbx\n",
    "import pandas as pd\n",
    "from typing import NamedTuple\n",
    "from tqdm.auto import tqdm\n",
    "import haiku as hk\n",
    "import jax\n",
    "from jax import random, jit, vmap, tree_map, lax\n",
    "from jax_tqdm import loop_tqdm\n",
    "import jax.numpy as jnp\n",
    "import plotly.express as px\n",
    "import optax\n",
    "import rlax\n",
    "import chex\n",
    "import gymnax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Data Structures***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_network_fn(num_outputs: int):\n",
    "    \"\"\"Define a fully connected multi-layer haiku network.\"\"\"\n",
    "\n",
    "    def network_fn(obs: chex.Array) -> chex.Array:\n",
    "        conv = hk.Conv2D(output_channels=16, kernel_shape=3, stride=1)\n",
    "        fc = hk.nets.MLP(\n",
    "            output_sizes=[128, num_outputs],\n",
    "            activation=jax.nn.relu,\n",
    "            activate_final=False,\n",
    "        )\n",
    "        x = jax.nn.relu(conv(obs))\n",
    "        x = x.reshape(-1)\n",
    "        x = fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    return hk.without_apply_rng(hk.transform(network_fn))\n",
    "\n",
    "\n",
    "class TrainState(NamedTuple):\n",
    "    params: hk.Params\n",
    "    target_params: hk.Params\n",
    "    opt_state: optax.OptState\n",
    "\n",
    "\n",
    "@chex.dataclass(frozen=True)\n",
    "class TimeStep:\n",
    "    observation: chex.Array\n",
    "    action: chex.Array\n",
    "    discount: chex.Array\n",
    "    reward: chex.Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We specify our parameters\n",
    "env_id = \"Freeway-MinAtar\"\n",
    "seed = 1\n",
    "num_envs = 1\n",
    "\n",
    "total_timesteps = 3000\n",
    "learning_starts = 5_000\n",
    "train_frequency = 1\n",
    "target_network_frequency = 1000\n",
    "\n",
    "tau = 1.0\n",
    "learning_rate = 2.5e-4\n",
    "start_e = 1.0\n",
    "end_e = 0.1\n",
    "duration = 100_000\n",
    "gamma = 0.99\n",
    "\n",
    "buffer_params = {\n",
    "    \"max_length\": 100_000,\n",
    "    \"min_length\": 32,\n",
    "    \"sample_batch_size\": 32,\n",
    "    \"add_sequences\": False,\n",
    "    \"add_batch_size\": None,\n",
    "    \"priority_exponent\": 0.5,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "env, env_params = gymnax.make(env_id)\n",
    "num_actions = env.num_actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***DQN and Optimizer initialization***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ryanp\\anaconda3\\lib\\site-packages\\gymnax\\environments\\minatar\\freeway.py:283: UserWarning:\n",
      "\n",
      "Explicitly requested dtype <class 'jax.numpy.int64'> requested in array is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "\n",
      "c:\\Users\\ryanp\\anaconda3\\lib\\site-packages\\jax\\_src\\ops\\scatter.py:96: FutureWarning:\n",
      "\n",
      "scatter inputs have incompatible types: cannot safely cast value from dtype=int32 to dtype=bool with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "key = random.PRNGKey(seed)\n",
    "key, q_key = random.split(key, 2)\n",
    "\n",
    "q_network = get_network_fn(num_actions)\n",
    "optim = optax.adam(learning_rate=learning_rate)\n",
    "\n",
    "dummy_obs, dummy_env_state = env.reset(key)\n",
    "params = q_network.init(q_key, dummy_obs.astype(jnp.float32))\n",
    "opt_state = optim.init(params)\n",
    "q_state = TrainState(\n",
    "    params=params,\n",
    "    target_params=params,\n",
    "    opt_state=opt_state,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Flashbax Buffer initialization***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ryanp\\anaconda3\\lib\\site-packages\\flashbax\\buffers\\trajectory_buffer.py:473: UserWarning:\n",
      "\n",
      "Setting max_size dynamically sets the `max_length_time_axis` to be `max_size`//`add_batch_size = 100000`.This allows one to control exactly how many timesteps are stored in the buffer.Note that this overrides the `max_length_time_axis` argument.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "buffer = fbx.make_prioritised_flat_buffer(**buffer_params)\n",
    "buffer = buffer.replace(\n",
    "    init=jax.jit(buffer.init),\n",
    "    add=jax.jit(buffer.add, donate_argnums=0),\n",
    "    sample=jax.jit(buffer.sample),\n",
    "    can_sample=jax.jit(buffer.can_sample),\n",
    ")\n",
    "\n",
    "dummy_timestep = TimeStep(\n",
    "    observation=dummy_obs,\n",
    "    action=jnp.int32(0),\n",
    "    reward=jnp.float32(0.0),\n",
    "    discount=jnp.float32(0.0),\n",
    ")\n",
    "buffer_state = buffer.init(dummy_timestep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_schedule(start_e: float, end_e: float, duration: int, t: int):\n",
    "    \"\"\"Linear schedule function for the epsilon greedy exploration.\"\"\"\n",
    "    slope = (end_e - start_e) / duration\n",
    "    return jnp.maximum(slope * t + start_e, end_e)\n",
    "\n",
    "\n",
    "def huber_loss(x: chex.Array, delta: float = 1.0) -> chex.Array:\n",
    "    \"\"\"Huber loss, similar to L2 loss close to zero, L1 loss away from zero.\n",
    "\n",
    "    See \"Robust Estimation of a Location Parameter\" by Huber.\n",
    "    (https://projecteuclid.org/download/pdf_1/euclid.aoms/1177703732).\n",
    "\n",
    "    Args:\n",
    "      x: a vector of arbitrary shape.\n",
    "      delta: the bounds for the huber loss transformation, defaults at 1.\n",
    "\n",
    "    Note `grad(huber_loss(x))` is equivalent to `grad(0.5 * clip_gradient(x)**2)`.\n",
    "\n",
    "    Returns:\n",
    "      a vector of same shape of `x`.\n",
    "    \"\"\"\n",
    "    chex.assert_type(x, float)\n",
    "\n",
    "    # 0.5 * x^2                  if |x| <= d\n",
    "    # 0.5 * d^2 + d * (|x| - d)  if |x| > d\n",
    "    abs_x = jnp.abs(x)\n",
    "    quadratic = jnp.minimum(abs_x, delta)\n",
    "    # Same as max(abs_x - delta, 0) but avoids potentially doubling gradient.\n",
    "    linear = abs_x - quadratic\n",
    "    return 0.5 * quadratic**2 + delta * linear\n",
    "\n",
    "\n",
    "@jit\n",
    "def update(q_state: TrainState, batch: TimeStep):\n",
    "    \"\"\"\n",
    "    Computes the updated model parameters and optimizer states\n",
    "    for a batch of experience.\n",
    "    \"\"\"\n",
    "\n",
    "    def batch_apply(params: dict, observations: jnp.ndarray):\n",
    "        return vmap(q_network.apply, in_axes=(None, 0))(params, observations)\n",
    "\n",
    "    def loss_fn(params: dict, target_params: dict, batch):\n",
    "        \"\"\"Computes the Q-learning TD error for a batch of timesteps\"\"\"\n",
    "        q_tm1 = batch_apply(params, batch.first.observation)\n",
    "        a_tm1 = batch.first.action\n",
    "        r_t = batch.first.reward\n",
    "        d_t = batch.first.discount * gamma\n",
    "        q_t = batch_apply(target_params, batch.second.observation)\n",
    "        td_error = vmap(rlax.q_learning)(q_tm1, a_tm1, r_t, d_t, q_t)\n",
    "        \n",
    "        return jnp.mean(huber_loss(td_error))\n",
    "\n",
    "    loss, grads = jax.value_and_grad(loss_fn)(\n",
    "        q_state.params, q_state.target_params, batch\n",
    "    )\n",
    "    updates, new_opt_state = optim.update(grads, q_state.opt_state)\n",
    "    new_params = optax.apply_updates(q_state.params, updates)\n",
    "    q_state = q_state._replace(params=new_params, opt_state=new_opt_state)\n",
    "\n",
    "    return loss, q_state\n",
    "\n",
    "\n",
    "@jit\n",
    "def action_select_fn(q_state: TrainState, obs: TimeStep):\n",
    "    q_values = q_network.apply(q_state.params, obs)\n",
    "    action = jnp.argmax(q_values, axis=-1)\n",
    "\n",
    "    return action\n",
    "\n",
    "\n",
    "@jit\n",
    "def perform_update(\n",
    "    q_state: TrainState,\n",
    "    buffer_state,\n",
    "    sample_key: random.PRNGKey,\n",
    "):\n",
    "    data = buffer.sample(buffer_state, sample_key)\n",
    "    loss, q_state = update(q_state, data.experience)\n",
    "\n",
    "    return loss, q_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_buffer(\n",
    "    rng: random.PRNGKey,\n",
    "    total_timesteps: int,\n",
    "    q_state: TrainState,\n",
    "    buffer_state,\n",
    "):\n",
    "    def _conditional_reset(key):\n",
    "        key, subkey = random.split(key)\n",
    "        obs, env_state = env.reset(subkey)\n",
    "        return obs, env_state\n",
    "\n",
    "    @jit\n",
    "    @loop_tqdm(total_timesteps, print_rate=int(total_timesteps / 100))\n",
    "    def _fori_body(current_step: int, val: tuple):\n",
    "        (obs, env_state, buffer_state, rng) = val\n",
    "        rng, env_key, action_key, step_key = random.split(rng, num=4)\n",
    "\n",
    "        action = env.action_space(env_params).sample(action_key)\n",
    "        obs, env_state, reward, done, _ = env.step(step_key, env_state, action)\n",
    "\n",
    "        timestep = TimeStep(\n",
    "            observation=obs,\n",
    "            action=action,\n",
    "            reward=reward,\n",
    "            discount=lax.select(done, 0.0, 0.99),\n",
    "        )\n",
    "        buffer_state = buffer.add(buffer_state, timestep)\n",
    "\n",
    "        # reset if done\n",
    "        obs, env_state = lax.cond(\n",
    "            done,\n",
    "            lambda _: _conditional_reset(env_key),\n",
    "            lambda _: (obs, env_state),\n",
    "            operand=None,\n",
    "        )\n",
    "\n",
    "        return (obs, env_state, buffer_state, rng)\n",
    "\n",
    "    obs, env_state = env.reset(rng)\n",
    "    init_val = (obs, env_state, buffer_state, rng)\n",
    "    (obs, env_state, buffer_state, rng) = lax.fori_loop(\n",
    "        0, total_timesteps, _fori_body, init_val\n",
    "    )\n",
    "\n",
    "    return buffer_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ryanp\\anaconda3\\lib\\site-packages\\jax\\_src\\numpy\\array_methods.py:66: UserWarning:\n",
      "\n",
      "Explicitly requested dtype <class 'jax.numpy.int64'> requested in astype is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "\n",
      "c:\\Users\\ryanp\\anaconda3\\lib\\site-packages\\gymnax\\environments\\minatar\\freeway.py:283: UserWarning:\n",
      "\n",
      "Explicitly requested dtype <class 'jax.numpy.int64'> requested in array is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "\n",
      "c:\\Users\\ryanp\\anaconda3\\lib\\site-packages\\jax\\_src\\ops\\scatter.py:96: FutureWarning:\n",
      "\n",
      "scatter inputs have incompatible types: cannot safely cast value from dtype=int32 to dtype=bool with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40a78a4cfe3d4809986330329c791299",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "buffer_state = fill_buffer(random.PRNGKey(0), 5_000, q_state, buffer_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_shape(tree):\n",
    "    return tree_map(lambda x: x.shape, tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***GER***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unwrap_batch(batch):\n",
    "    \"\"\"\n",
    "    Converts a batch of experiences to batches of observations,\n",
    "    actions, rewards and done flags. \n",
    "    \"\"\"\n",
    "    obs_tm1 = batch.first.observation\n",
    "    obs_t = batch.second.observation\n",
    "    a_tm1 = batch.first.action\n",
    "    r_t = batch.first.reward\n",
    "    d_t = batch.first.discount * gamma\n",
    "\n",
    "    return {\n",
    "        \"obs_tm1\": obs_tm1,\n",
    "        \"obs_t\": obs_t,\n",
    "        \"a_tm1\": a_tm1,\n",
    "        \"r_t\": r_t,\n",
    "        \"d_t\": d_t,\n",
    "    }\n",
    "\n",
    "\n",
    "def single_sample_loss(params, target_params, obs_tm1, obs_t, a_tm1, r_t, d_t):\n",
    "    q_tm1 = q_network.apply(params, obs_tm1)\n",
    "    q_t = q_network.apply(target_params, obs_t)\n",
    "    td_error = rlax.q_learning(q_tm1, a_tm1, r_t, d_t, q_t)\n",
    "\n",
    "    return huber_loss(td_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conv2_d': {'b': (32, 16), 'w': (32, 3, 3, 7, 16)},\n",
       " 'mlp/~/linear_0': {'b': (32, 128), 'w': (32, 1600, 128)},\n",
       " 'mlp/~/linear_1': {'b': (32, 3), 'w': (32, 128, 3)}}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = buffer.sample(buffer_state, random.PRNGKey(0))\n",
    "per_sample_loss, per_sample_grads = vmap(\n",
    "    jax.value_and_grad(single_sample_loss), in_axes=(None, None)\n",
    ")(\n",
    "    q_state.params,\n",
    "    q_state.target_params,\n",
    "    **unwrap_batch(data.experience),\n",
    ")\n",
    "tree_shape(per_sample_grads) # all parameters have an additional batch dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conv2_d': {'b': Array([ 0.0024511 ,  0.00449101,  0.00045069, -0.00088667,  0.00264114,\n",
       "         -0.00114079,  0.00130348, -0.00130828,  0.00205221,  0.00036082,\n",
       "          0.00605653,  0.00343748,  0.00446917,  0.00139386, -0.00043764,\n",
       "          0.00256009], dtype=float32),\n",
       "  'w': Array([[[[ 2.29266356e-04, -8.42906593e-05, -1.05393774e-05, ...,\n",
       "            -2.49934783e-05, -7.44493518e-05,  5.17422777e-05],\n",
       "           [ 9.14558070e-04,  1.74292261e-04, -5.73871890e-04, ...,\n",
       "            -7.32992339e-05,  4.10810782e-04,  4.71305015e-04],\n",
       "           [-9.74045979e-05,  4.58379625e-04,  2.31650527e-04, ...,\n",
       "             8.21635331e-05,  0.00000000e+00, -4.73958280e-05],\n",
       "           ...,\n",
       "           [ 1.11865251e-04,  6.87608117e-05,  7.15275091e-05, ...,\n",
       "            -2.90360884e-04,  0.00000000e+00,  1.70633339e-05],\n",
       "           [ 4.08641281e-05,  1.11928784e-05, -4.21751320e-05, ...,\n",
       "             1.35443497e-05,  1.95196801e-04, -1.16156982e-04],\n",
       "           [-2.30927428e-04,  1.19236145e-04, -8.04105512e-05, ...,\n",
       "            -1.32453832e-04, -3.10274991e-05, -8.00413254e-05]],\n",
       "  \n",
       "          [[ 1.86154881e-04,  2.75678576e-05, -2.27672608e-05, ...,\n",
       "             3.82644852e-04,  1.66442333e-05,  1.08917848e-05],\n",
       "           [-3.70568392e-04,  1.04131934e-03,  4.99970978e-04, ...,\n",
       "             6.23743748e-04,  8.60502914e-05,  1.13685783e-05],\n",
       "           [ 4.68103062e-05,  7.62800191e-05, -1.95339933e-04, ...,\n",
       "             2.16498593e-05, -1.01314208e-06,  1.18406213e-04],\n",
       "           ...,\n",
       "           [ 5.51041448e-04,  2.34278181e-04, -4.07755957e-04, ...,\n",
       "            -1.36044458e-04,  4.44832767e-05,  5.32569713e-04],\n",
       "           [ 3.66177090e-04,  1.69788400e-05,  6.49017384e-05, ...,\n",
       "             1.33545600e-05,  3.42507774e-05,  7.68961108e-05],\n",
       "           [ 2.87982519e-04, -3.41975945e-04,  9.71058835e-05, ...,\n",
       "            -3.90527093e-05,  1.26257684e-04, -1.99226051e-05]],\n",
       "  \n",
       "          [[-9.55884389e-05,  3.00858374e-04, -2.54605373e-04, ...,\n",
       "            -6.65892658e-05, -1.03327424e-04,  1.60045784e-06],\n",
       "           [ 6.56264019e-04,  8.59336986e-04, -6.30131035e-05, ...,\n",
       "             6.40355400e-04,  8.77151688e-05,  8.54147002e-05],\n",
       "           [-4.15049290e-05, -8.24801464e-05, -3.53285141e-04, ...,\n",
       "             7.42963530e-05,  8.20120840e-05,  1.17911404e-04],\n",
       "           ...,\n",
       "           [-5.63866197e-05,  1.97135101e-04, -5.07820587e-05, ...,\n",
       "             1.55040005e-04, -9.80847472e-06,  7.74380824e-05],\n",
       "           [-2.43808245e-04,  1.85333511e-06, -1.16808369e-04, ...,\n",
       "            -2.48826138e-04, -4.55035988e-05, -6.62422390e-05],\n",
       "           [ 1.10745546e-04,  2.62746071e-05,  5.33881102e-05, ...,\n",
       "            -1.01189973e-04, -2.71068020e-05,  9.68450549e-05]]],\n",
       "  \n",
       "  \n",
       "         [[[ 1.50773276e-05,  5.36809443e-04,  4.42958626e-05, ...,\n",
       "             1.13155584e-04, -1.15541436e-04,  2.88135081e-04],\n",
       "           [ 6.18696155e-04, -5.03238291e-04, -3.89024557e-04, ...,\n",
       "             4.04358550e-04,  5.55054285e-04,  9.64255014e-05],\n",
       "           [ 3.37425357e-04,  1.63805598e-04,  3.39235630e-05, ...,\n",
       "            -5.53201135e-05, -1.31188426e-06,  3.85205378e-04],\n",
       "           ...,\n",
       "           [ 2.07559788e-04,  4.89444530e-04, -2.85932270e-04, ...,\n",
       "            -2.01922448e-04, -4.96642751e-05, -2.70506389e-05],\n",
       "           [-4.53403831e-04, -2.59107237e-05,  5.85116504e-04, ...,\n",
       "             1.16575582e-04,  9.88713509e-05,  2.72391335e-04],\n",
       "           [ 1.13833739e-04, -9.09619703e-05, -2.26531138e-05, ...,\n",
       "            -8.55856124e-05, -1.66840418e-05,  5.62181958e-06]],\n",
       "  \n",
       "          [[-1.06985299e-05,  1.52613866e-04, -7.57909074e-05, ...,\n",
       "             2.89196847e-04,  3.32203854e-05, -3.43078063e-05],\n",
       "           [-7.97142638e-05,  1.51754054e-03, -9.18404694e-05, ...,\n",
       "            -4.01993835e-04,  2.92206441e-05, -3.00022948e-04],\n",
       "           [-2.06303288e-04,  1.09604443e-04, -1.16257841e-04, ...,\n",
       "             5.14063169e-04, -8.73665631e-05, -9.14179473e-05],\n",
       "           ...,\n",
       "           [ 5.35317173e-04,  2.96431041e-04, -4.99525282e-04, ...,\n",
       "             7.65977748e-05,  2.18799149e-04,  8.82502762e-04],\n",
       "           [ 9.36895449e-05,  1.43773868e-04,  2.30295904e-04, ...,\n",
       "             1.03382423e-04, -6.98026706e-05,  2.23108837e-05],\n",
       "           [ 4.08031075e-04,  5.49713441e-04,  6.67586282e-05, ...,\n",
       "             1.09345565e-05,  3.33831616e-04, -2.67535452e-05]],\n",
       "  \n",
       "          [[ 4.63755496e-05, -8.35530518e-05, -1.35700669e-04, ...,\n",
       "             2.00159939e-05,  7.19572636e-05,  1.13276430e-04],\n",
       "           [ 3.26694892e-04,  1.15706341e-03, -1.56332884e-04, ...,\n",
       "             5.58114378e-04,  7.78816393e-05, -2.13042822e-05],\n",
       "           [-1.09445951e-04,  1.47962652e-04,  3.52819552e-05, ...,\n",
       "             2.12328996e-06,  1.21935227e-04, -1.43265963e-04],\n",
       "           ...,\n",
       "           [-1.08314780e-04, -1.37021743e-05,  2.54674069e-06, ...,\n",
       "            -6.32684387e-05,  7.93061699e-05,  2.73864891e-04],\n",
       "           [ 4.18332056e-05,  2.84541238e-05,  2.66455056e-04, ...,\n",
       "            -3.31245421e-04,  2.55918349e-05,  8.92216412e-05],\n",
       "           [ 2.79595319e-04,  3.64735519e-04, -9.83631544e-05, ...,\n",
       "             2.19317590e-04,  2.79177257e-05, -6.48204878e-05]]],\n",
       "  \n",
       "  \n",
       "         [[[ 5.04150346e-04,  1.02548831e-04,  1.69176754e-04, ...,\n",
       "             0.00000000e+00,  3.69138470e-05,  1.84971665e-04],\n",
       "           [ 3.19540472e-04,  3.09108873e-05, -4.93668544e-04, ...,\n",
       "            -7.65864679e-05, -9.06152360e-04, -3.93301510e-04],\n",
       "           [ 3.25595960e-04, -1.40273478e-04, -5.80542946e-05, ...,\n",
       "            -1.54479611e-04, -7.47348577e-06, -3.35703196e-04],\n",
       "           ...,\n",
       "           [ 3.89801105e-04, -1.14182061e-04,  4.80284616e-05, ...,\n",
       "            -1.35268683e-05,  5.54932740e-05,  3.18316634e-05],\n",
       "           [ 2.32560924e-04,  1.83973607e-04, -2.49606092e-05, ...,\n",
       "             0.00000000e+00,  2.34476684e-04, -2.67933374e-05],\n",
       "           [-2.18050227e-05,  6.30392460e-05, -1.27499603e-04, ...,\n",
       "             2.30536807e-05, -1.21892706e-04, -1.10169414e-04]],\n",
       "  \n",
       "          [[ 3.04357847e-04,  2.45154697e-05, -1.78188580e-04, ...,\n",
       "             1.65337224e-05, -6.39657628e-06,  8.58014537e-05],\n",
       "           [-6.90577726e-04, -2.04291166e-04,  2.83502915e-04, ...,\n",
       "             2.38217035e-05,  7.03885336e-04,  9.13379306e-04],\n",
       "           [-1.39249198e-04,  4.82350370e-05, -6.24747481e-05, ...,\n",
       "             5.55137740e-05,  3.01827131e-05, -1.09613960e-04],\n",
       "           ...,\n",
       "           [ 3.66132357e-04, -8.44032475e-05, -3.93562252e-04, ...,\n",
       "             3.85529827e-04, -5.49271004e-04,  1.24424740e-04],\n",
       "           [-1.67668462e-04,  1.28191081e-04, -4.89762533e-05, ...,\n",
       "            -1.31035544e-04,  0.00000000e+00, -1.41308308e-04],\n",
       "           [ 2.87250703e-04,  4.90896287e-04,  1.44889473e-05, ...,\n",
       "            -6.10307070e-05, -1.31759560e-04,  2.25800613e-04]],\n",
       "  \n",
       "          [[ 1.98679656e-04,  3.50677947e-05,  6.43570747e-05, ...,\n",
       "            -2.56898202e-05,  1.01730038e-05,  9.71908084e-05],\n",
       "           [ 7.02949648e-04,  5.48353826e-04,  2.37396715e-04, ...,\n",
       "             1.04730972e-03,  6.22303560e-07,  5.17540029e-04],\n",
       "           [-3.52470262e-04,  3.27127018e-05, -1.10668188e-04, ...,\n",
       "             1.16484349e-04,  5.77261198e-05,  1.86169200e-04],\n",
       "           ...,\n",
       "           [-5.30559482e-05,  1.65997932e-04, -2.55678788e-05, ...,\n",
       "             7.27418665e-05,  1.89011189e-04,  1.57582617e-04],\n",
       "           [ 2.77590210e-04,  1.09427419e-04,  1.70940548e-05, ...,\n",
       "            -4.82729301e-05,  8.69639844e-05,  1.11598201e-04],\n",
       "           [-2.16872082e-04, -1.99405215e-04, -4.29138599e-05, ...,\n",
       "             6.95655308e-06, -1.88109843e-04,  1.70318512e-04]]]],      dtype=float32)},\n",
       " 'mlp/~/linear_0': {'b': Array([ 1.3480950e-03,  1.3581601e-03, -3.1398833e-04,  1.5683322e-03,\n",
       "         -3.1353272e-03, -2.3910792e-04, -1.7751607e-03, -6.9255102e-04,\n",
       "         -6.3918688e-04, -8.1192667e-04,  1.2361755e-03, -5.7604152e-04,\n",
       "         -1.4720135e-06, -7.9307053e-04, -3.9567165e-03, -1.2213689e-03,\n",
       "         -3.3416436e-05, -3.2956718e-04,  1.8282202e-03,  2.3888242e-03,\n",
       "          2.9442871e-03, -3.6385623e-03, -5.1935564e-04,  2.2297259e-03,\n",
       "          3.8407163e-03, -1.6662664e-03,  2.9835165e-03,  2.8592281e-04,\n",
       "          1.6980634e-04,  8.6443190e-04, -9.1663020e-04,  5.2102576e-03,\n",
       "          1.9292394e-03,  6.0493229e-03,  5.8064883e-04,  9.6520933e-04,\n",
       "          6.1278499e-04, -5.0345954e-04,  8.8886009e-06, -2.7270686e-05,\n",
       "          2.7823423e-03,  1.9856782e-03, -1.1303741e-03,  2.5698545e-03,\n",
       "          3.4716320e-03, -1.7166649e-03, -9.1314723e-04,  2.4665503e-03,\n",
       "          2.4789721e-03,  1.0476208e-03, -2.4999182e-03, -2.6090031e-03,\n",
       "          2.5569862e-03,  1.2673475e-03,  1.2098087e-03,  2.4920673e-04,\n",
       "          2.1816828e-04, -6.9507179e-05, -2.5613452e-04, -1.6778652e-04,\n",
       "         -1.0457097e-05, -9.3121652e-04, -9.0258545e-04, -4.7898118e-04,\n",
       "         -3.6034596e-03,  1.3050024e-03, -1.5074540e-04, -5.1640600e-05,\n",
       "         -1.1243039e-04, -2.7349177e-03, -5.3917598e-03,  1.0588457e-03,\n",
       "          4.0959218e-03,  1.4850204e-03,  2.2659649e-03,  2.6251117e-03,\n",
       "         -2.0792570e-03,  4.2765072e-04,  1.4352798e-03,  1.8436456e-04,\n",
       "         -2.0747034e-04, -2.3329800e-03, -6.5015280e-05, -1.0703605e-03,\n",
       "         -6.7864393e-04, -5.1619986e-04, -3.0730854e-04,  2.0174475e-03,\n",
       "          2.2059185e-03,  4.6258341e-03, -3.7922166e-04,  1.5261066e-04,\n",
       "          1.3415997e-03,  1.7827251e-03,  1.2480079e-04,  7.1887148e-04,\n",
       "          3.9521800e-03,  2.5005594e-03, -1.0016984e-03, -2.7745869e-03,\n",
       "         -5.1480439e-04,  1.9023526e-03, -1.3001108e-03, -2.7975361e-03,\n",
       "          1.7435750e-03, -2.4890448e-03,  7.0444665e-05,  1.2391101e-03,\n",
       "          1.4499114e-03, -1.5434422e-04,  1.7836080e-03, -1.6654887e-03,\n",
       "          3.0641066e-04, -7.3565543e-04, -3.6178138e-03,  2.2635066e-03,\n",
       "         -1.4606287e-03, -4.6880337e-04,  2.2294128e-03,  2.0599235e-03,\n",
       "          2.5284477e-03, -1.7178878e-03, -1.2259473e-04,  1.8212827e-03,\n",
       "          5.7125813e-05, -5.8949011e-04, -2.9889448e-03,  1.8701332e-03],      dtype=float32),\n",
       "  'w': Array([[ 5.49227261e-05, -7.26641156e-06, -6.04637862e-05, ...,\n",
       "          -6.18768681e-05, -2.25604381e-04,  5.27399279e-05],\n",
       "         [ 9.75463845e-05, -2.54887641e-06, -2.93624926e-05, ...,\n",
       "          -2.33146857e-05, -1.13748138e-04,  4.30442451e-05],\n",
       "         [-2.92973795e-07,  1.39785616e-05,  3.09616553e-07, ...,\n",
       "          -1.03908303e-06, -1.78003938e-05,  1.17997261e-05],\n",
       "         ...,\n",
       "         [ 7.18215324e-06,  1.13581655e-05, -6.13962948e-06, ...,\n",
       "          -6.43559315e-06, -2.53838080e-05, -8.12788869e-07],\n",
       "         [ 1.20208197e-05,  1.40711281e-05,  5.23479684e-06, ...,\n",
       "          -1.72443706e-05, -4.44298093e-05,  4.82695796e-05],\n",
       "         [ 3.31567094e-07, -4.49562231e-06,  0.00000000e+00, ...,\n",
       "           1.52023276e-07, -6.06737672e-07,  3.71279179e-06]],      dtype=float32)},\n",
       " 'mlp/~/linear_1': {'b': Array([-0.02031215, -0.03315198, -0.00757675], dtype=float32),\n",
       "  'w': Array([[-2.79869710e-04, -5.83900022e-04, -5.28286444e-04],\n",
       "         [-9.00970539e-04, -3.21360421e-04, -3.50469259e-06],\n",
       "         [-2.53693899e-04, -9.28157242e-04, -7.18654046e-05],\n",
       "         [-9.52787348e-04, -1.65041327e-03, -7.89500482e-04],\n",
       "         [-8.18295637e-04, -1.08279986e-03, -2.93514400e-04],\n",
       "         [-5.95300335e-05, -1.15523151e-04, -1.62409560e-04],\n",
       "         [-3.98223085e-04, -6.74752810e-04, -1.58141688e-06],\n",
       "         [-2.45866162e-04, -1.27340158e-04,  5.14291685e-07],\n",
       "         [-3.20262159e-04, -1.47489685e-04,  3.15398665e-06],\n",
       "         [-5.69286291e-04, -8.95228470e-04, -3.22759966e-04],\n",
       "         [-1.21449679e-03, -2.25687260e-03, -4.38133313e-04],\n",
       "         [-1.42649643e-03, -9.61772224e-04, -2.87189265e-04],\n",
       "         [-5.18720888e-04, -2.11651975e-04, -1.53697139e-04],\n",
       "         [-6.41599938e-04, -1.03771838e-03, -2.32746417e-04],\n",
       "         [-1.19043840e-03, -3.01122712e-03, -1.52572029e-04],\n",
       "         [-1.71891617e-04, -7.90635066e-04, -2.21624403e-04],\n",
       "         [ 4.77112371e-06, -3.67183093e-04,  8.09796838e-05],\n",
       "         [-5.16066619e-04, -2.15222454e-03, -7.52388849e-04],\n",
       "         [-8.32325546e-04, -3.56170582e-03, -6.90376444e-04],\n",
       "         [-4.99143382e-04, -8.83906730e-04, -4.22989106e-05],\n",
       "         [-2.45920219e-03, -2.30527134e-03, -6.85404637e-04],\n",
       "         [-7.71646795e-04, -2.22383579e-03,  1.30254739e-06],\n",
       "         [-2.96965678e-04, -3.49747686e-04, -2.41987931e-04],\n",
       "         [-5.65245573e-04, -2.25261203e-03, -1.68054146e-04],\n",
       "         [-5.56788873e-04, -3.66412615e-03, -2.94308935e-04],\n",
       "         [-7.88639372e-05, -8.46466806e-04, -5.16230793e-05],\n",
       "         [-4.50527587e-04, -2.14237347e-03, -5.39369881e-04],\n",
       "         [-1.12955412e-03, -9.42382438e-04, -1.16590825e-04],\n",
       "         [-4.44674108e-04, -6.33581658e-05, -1.94541484e-04],\n",
       "         [-6.60904858e-04, -1.19638594e-03, -7.38525487e-05],\n",
       "         [-2.24012663e-04, -4.64600453e-04, -6.82133425e-04],\n",
       "         [-1.55140716e-03, -3.26782255e-03, -8.07404285e-05],\n",
       "         [-5.55525301e-04, -6.31910050e-04, -3.49150469e-05],\n",
       "         [-2.41312408e-03, -5.58921462e-03, -6.76100317e-04],\n",
       "         [-1.06569991e-04, -1.95823795e-05, -1.70138766e-04],\n",
       "         [-1.01988856e-03, -5.08588157e-04, -9.62114427e-07],\n",
       "         [-4.26094542e-04, -6.39563776e-04, -9.18983278e-05],\n",
       "         [ 1.93057349e-05, -1.63996621e-04,  1.85933663e-06],\n",
       "         [-8.55389459e-04, -1.96924922e-03, -6.17007900e-04],\n",
       "         [-2.68121919e-04, -1.12399604e-04,  0.00000000e+00],\n",
       "         [-9.81862977e-05, -8.18505534e-04, -1.40530945e-04],\n",
       "         [-7.34270085e-04, -8.90532683e-04, -3.21776082e-04],\n",
       "         [-1.42704172e-04, -2.63010152e-04,  1.39246267e-05],\n",
       "         [-6.38127793e-04, -1.31249614e-03, -4.08337946e-04],\n",
       "         [-2.34537059e-04, -1.52283174e-03, -9.61065758e-04],\n",
       "         [-4.05081490e-04, -7.43257347e-04, -4.64801065e-04],\n",
       "         [-1.77214999e-04, -8.14125524e-04, -4.76516521e-04],\n",
       "         [-6.24175998e-04, -2.02590250e-03, -2.80562344e-05],\n",
       "         [-6.72934053e-04, -1.08500081e-03, -8.08409357e-04],\n",
       "         [-8.11728474e-04, -1.15366257e-03, -2.23235416e-04],\n",
       "         [-3.22391192e-04, -5.02089912e-04, -4.57149843e-04],\n",
       "         [-2.37044413e-04, -3.60852195e-04, -3.20530889e-05],\n",
       "         [-2.81913718e-03, -2.66844896e-03, -2.45355244e-04],\n",
       "         [-3.87199456e-04, -7.71573803e-04, -2.98250234e-04],\n",
       "         [-3.49942507e-04, -1.01261109e-03, -4.36179456e-04],\n",
       "         [ 0.00000000e+00, -3.18986567e-05, -2.46771757e-04],\n",
       "         [-2.31859216e-04, -3.22647211e-05, -7.67173406e-05],\n",
       "         [-5.86296956e-04, -1.43790676e-03, -2.98225379e-04],\n",
       "         [-1.30380664e-04, -5.08334379e-05, -1.31688183e-04],\n",
       "         [ 2.07483681e-05, -9.84628568e-05,  0.00000000e+00],\n",
       "         [-2.99560616e-05,  0.00000000e+00,  0.00000000e+00],\n",
       "         [-5.14864747e-04, -4.80005459e-04, -1.98909092e-05],\n",
       "         [-4.43195131e-05, -6.93231064e-04,  5.94045705e-05],\n",
       "         [-5.75638667e-04, -4.81201743e-04, -5.69758820e-04],\n",
       "         [-8.37642758e-04, -1.17777428e-03, -1.73566572e-04],\n",
       "         [-1.12457946e-03, -1.78169634e-03, -5.40507550e-04],\n",
       "         [-8.28266900e-04, -7.62984972e-04, -7.13709014e-05],\n",
       "         [ 4.25846520e-05, -1.59717817e-03, -7.17169605e-05],\n",
       "         [-2.19209527e-04, -5.26353506e-06,  1.88775157e-05],\n",
       "         [-1.46537798e-03, -3.75140854e-03, -8.99981533e-04],\n",
       "         [-1.14745146e-03, -2.14944663e-03, -5.54931816e-04],\n",
       "         [-8.49194650e-04, -1.31230953e-03, -5.87557850e-04],\n",
       "         [-9.75783041e-04, -3.86461918e-03, -4.45945421e-04],\n",
       "         [-6.18523045e-04, -4.43880213e-04, -4.11670306e-04],\n",
       "         [-1.39613089e-03, -1.08421571e-03,  1.18873031e-05],\n",
       "         [-1.01659028e-03, -7.72932137e-04,  8.66216433e-05],\n",
       "         [-9.95797338e-04, -1.21575978e-03, -2.30068865e-04],\n",
       "         [-2.65086535e-04, -6.41453255e-04,  6.90646630e-05],\n",
       "         [-2.80888256e-04, -3.29327362e-04, -3.04227084e-04],\n",
       "         [-3.95291427e-05, -8.22124828e-04, -2.38095236e-04],\n",
       "         [-2.34553299e-04, -2.94511810e-05, -2.01619987e-05],\n",
       "         [-8.00793874e-04, -1.76860136e-03, -2.03314034e-04],\n",
       "         [-1.89254759e-04, -1.40645006e-03, -2.72116813e-05],\n",
       "         [-5.53713355e-04, -4.89729457e-04, -1.06874155e-04],\n",
       "         [-7.84436124e-04, -8.70366464e-04, -1.01321115e-04],\n",
       "         [-1.69702747e-04, -7.84739314e-05,  8.10450074e-05],\n",
       "         [ 9.15546480e-06, -2.71307945e-04, -9.19377853e-05],\n",
       "         [-2.00757524e-03, -3.38745723e-03, -3.31378309e-04],\n",
       "         [-1.27319887e-03, -8.44263704e-04, -2.03862015e-04],\n",
       "         [-1.61614246e-03, -1.05569547e-03,  1.65201236e-05],\n",
       "         [ 1.06803280e-04, -1.70822357e-04, -1.86842299e-05],\n",
       "         [ 0.00000000e+00, -5.66346571e-04,  9.87539806e-06],\n",
       "         [-9.60626785e-05, -4.10652538e-05, -1.18627977e-04],\n",
       "         [-1.12756905e-04, -5.66206116e-04, -3.03400971e-04],\n",
       "         [ 0.00000000e+00, -7.30008396e-05, -2.51557594e-06],\n",
       "         [-4.58943774e-04, -2.37665488e-03, -6.13675453e-04],\n",
       "         [-1.88999996e-03, -2.18105456e-03, -3.53038631e-04],\n",
       "         [-1.80326484e-03, -2.79141706e-03, -1.26696238e-03],\n",
       "         [-1.65234582e-04, -1.02793507e-04, -5.20559252e-06],\n",
       "         [-1.29764667e-03, -1.02231873e-03, -5.44746930e-04],\n",
       "         [-1.03977241e-03, -6.58848672e-04, -7.91037106e-04],\n",
       "         [-8.50573997e-04, -1.04619970e-03, -7.72828789e-05],\n",
       "         [-1.74945453e-03, -2.74125906e-03, -4.26520011e-04],\n",
       "         [-9.43657127e-04, -5.92031342e-04, -6.00403000e-04],\n",
       "         [-6.38175348e-04, -5.91264747e-04, -1.77878992e-05],\n",
       "         [-9.26820619e-04, -2.23912671e-03, -2.06618512e-04],\n",
       "         [ 6.20758728e-05,  0.00000000e+00, -1.38559830e-04],\n",
       "         [-7.33499415e-04, -9.47060180e-04, -2.51594000e-04],\n",
       "         [-1.26091379e-03, -9.48363217e-04, -4.36985283e-04],\n",
       "         [-2.12007406e-04, -3.57102923e-04, -9.95878654e-05],\n",
       "         [-4.13272355e-04, -9.54616553e-06, -5.15490305e-04],\n",
       "         [-2.92920013e-04, -8.34747858e-04, -5.00556635e-05],\n",
       "         [-1.01681435e-04, -1.36341201e-04, -1.86308389e-04],\n",
       "         [-1.53452449e-04, -8.51316599e-06, -4.02728147e-05],\n",
       "         [-8.65030452e-04, -9.43563937e-04, -1.43160738e-04],\n",
       "         [-5.92139317e-04, -6.95824157e-04, -6.66303094e-07],\n",
       "         [-7.96159264e-04, -2.53603654e-03, -1.76693749e-04],\n",
       "         [-8.79956278e-05, -9.38841375e-04, -2.04404903e-04],\n",
       "         [-1.26000925e-03, -2.71537364e-03, -2.09479564e-04],\n",
       "         [-2.52936245e-03, -2.22122064e-03, -7.21180550e-06],\n",
       "         [-1.23085396e-03, -2.09249044e-03, -3.16733785e-04],\n",
       "         [-5.24334551e-04, -9.40989878e-04, -5.81358792e-04],\n",
       "         [-2.47465476e-04, -1.02237449e-03, -2.75392027e-04],\n",
       "         [-1.62041091e-04, -1.39174878e-03, -5.93904988e-04],\n",
       "         [-1.63373142e-03, -2.60807481e-03, -1.56683338e-04],\n",
       "         [-1.79230134e-04, -5.27038821e-04, -1.86501551e-04],\n",
       "         [-1.04019442e-03, -8.35405488e-04, -5.41545276e-04],\n",
       "         [-9.85577353e-04, -1.16084469e-03, -2.11429448e-04]],      dtype=float32)}}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads_per = tree_map(\n",
    "    lambda x: jnp.mean(x, axis=0),\n",
    "    per_sample_grads,\n",
    ")\n",
    "grads_per"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conv2_d': {'b': (32,), 'w': (32,)},\n",
       " 'mlp/~/linear_0': {'b': (32,), 'w': (32,)},\n",
       " 'mlp/~/linear_1': {'b': (32,), 'w': (32,)}}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "per_sample_norms = tree_map(\n",
    "    lambda x: jnp.sqrt(\n",
    "        jnp.sum(\n",
    "            x**2,\n",
    "            axis=tuple(range(1, x.ndim)), # sum over all dims except batch dim\n",
    "        ),\n",
    "    ),\n",
    "    per_sample_grads,\n",
    ")\n",
    "\n",
    "tree_shape(per_sample_norms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([0.06935442, 0.01756447, 0.01155434, 0.03667615, 0.06808832,\n",
       "       0.03059145, 0.08253931, 0.06296004, 0.00396752, 0.04660731,\n",
       "       0.12405833, 0.05548818, 0.00023469, 0.10483947, 0.03016827,\n",
       "       0.12912917, 0.09218165, 0.06446829, 0.0802656 , 0.09330662,\n",
       "       0.09774294, 0.00037563, 0.01540068, 0.00419724, 0.08082651,\n",
       "       0.03862018, 0.15389241, 0.14672226, 0.03395448, 0.0530236 ,\n",
       "       0.1369601 , 0.03773179], dtype=float32)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree, _ = jax.tree_util.tree_flatten(per_sample_norms)\n",
    "jnp.array(tree).mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Normal Update***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def update(q_state: TrainState, batch: TimeStep):\n",
    "    \"\"\"\n",
    "    Computes the updated model parameters and optimizer states\n",
    "    for a batch of experience.\n",
    "    \"\"\"\n",
    "\n",
    "    def batch_apply(params: dict, observations: jnp.ndarray):\n",
    "        return vmap(q_network.apply, in_axes=(None, 0))(params, observations)\n",
    "\n",
    "    def loss_fn(params: dict, target_params: dict, batch):\n",
    "        \"\"\"Computes the Q-learning TD error for a batch of timesteps\"\"\"\n",
    "        q_tm1 = batch_apply(params, batch.first.observation)\n",
    "        a_tm1 = batch.first.action\n",
    "        r_t = batch.first.reward\n",
    "        d_t = batch.first.discount * gamma\n",
    "        q_t = batch_apply(target_params, batch.second.observation)\n",
    "        td_error = vmap(rlax.q_learning)(q_tm1, a_tm1, r_t, d_t, q_t)\n",
    "        \n",
    "        return jnp.mean(huber_loss(td_error))\n",
    "\n",
    "    loss, grads = jax.value_and_grad(loss_fn)(\n",
    "        q_state.params, q_state.target_params, batch\n",
    "    )\n",
    "    updates, new_opt_state = optim.update(grads, q_state.opt_state)\n",
    "    new_params = optax.apply_updates(q_state.params, updates)\n",
    "    q_state = q_state._replace(params=new_params, opt_state=new_opt_state)\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conv2_d': {'b': (16,), 'w': (3, 3, 7, 16)},\n",
       " 'mlp/~/linear_0': {'b': (128,), 'w': (1600, 128)},\n",
       " 'mlp/~/linear_1': {'b': (3,), 'w': (128, 3)}}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads = update(q_state, data.experience)\n",
    "tree_shape(grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Checking closeness between PER updates and DQN gradients***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some elements are not exactly equal (maybe because of rounding ?)\n",
    "chex.assert_trees_all_close(grads, grads_per, rtol=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when taking the gradient means, we get equality with 1e-6 precision\n",
    "grads_means = tree_map(lambda x: jnp.mean(x), grads)\n",
    "grads_per_means = tree_map(lambda x: jnp.mean(x), grads_per)\n",
    "chex.assert_trees_all_close(grads_means, grads_per_means, rtol=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Full GER update function***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def ger_update(q_state: TrainState, buffer_state, batch: TimeStep):\n",
    "    \"\"\"\n",
    "    Computes the updated model parameters and optimizer states\n",
    "    for a batch of experience.\n",
    "    \"\"\"\n",
    "\n",
    "    def unwrap_batch(batch: TimeStep):\n",
    "        \"\"\"\n",
    "        Converts a batch of experiences to batches of observations,\n",
    "        actions, rewards and done flags.\n",
    "        \"\"\"\n",
    "        obs_tm1 = batch.first.observation\n",
    "        obs_t = batch.second.observation\n",
    "        a_tm1 = batch.first.action\n",
    "        r_t = batch.first.reward\n",
    "        d_t = batch.first.discount * gamma\n",
    "\n",
    "        return {\n",
    "            \"obs_tm1\": obs_tm1,\n",
    "            \"obs_t\": obs_t,\n",
    "            \"a_tm1\": a_tm1,\n",
    "            \"r_t\": r_t,\n",
    "            \"d_t\": d_t,\n",
    "        }\n",
    "\n",
    "    def single_sample_loss(params, target_params, obs_tm1, obs_t, a_tm1, r_t, d_t):\n",
    "        \"\"\"Returns the huber loss of a single experience.\"\"\"\n",
    "        q_tm1 = q_network.apply(params, obs_tm1)\n",
    "        q_t = q_network.apply(target_params, obs_t)\n",
    "        td_error = rlax.q_learning(q_tm1, a_tm1, r_t, d_t, q_t)\n",
    "\n",
    "        return huber_loss(td_error)\n",
    "\n",
    "    # get per-sample losses and gradients\n",
    "    per_sample_loss, per_sample_grads = vmap(\n",
    "        jax.value_and_grad(single_sample_loss), in_axes=(None, None)\n",
    "    )(q_state.params, q_state.target_params, **unwrap_batch(batch.experience))\n",
    "\n",
    "    # compute per-sample gradient norms\n",
    "    per_sample_norms = tree_map(\n",
    "        lambda x: jnp.sqrt(\n",
    "            jnp.sum(\n",
    "                x**2,\n",
    "                axis=tuple(range(1, x.ndim)),  # sum over all dims except batch dim\n",
    "            ),\n",
    "        ),\n",
    "        per_sample_grads,\n",
    "    )\n",
    "    per_sample_norms, _ = jax.tree_util.tree_flatten(per_sample_norms)\n",
    "    per_sample_norms = jnp.array(per_sample_norms).mean(axis=0)\n",
    "\n",
    "    # importance sampling weights\n",
    "    importance_weights = (1.0 / batch.priorities).astype(jnp.float32)\n",
    "    importance_weights **= 0.5  # beta\n",
    "    importance_weights /= jnp.max(importance_weights)\n",
    "\n",
    "    # updating priorities\n",
    "    new_priorities = per_sample_norms / importance_weights\n",
    "\n",
    "    # grads = average per-sample grads across the batch dimension\n",
    "    grads = tree_map(lambda x: jnp.mean(x, axis=0), per_sample_grads)\n",
    "    # loss = average per-sample loss weighted by importance\n",
    "    loss = jnp.mean(per_sample_loss * importance_weights)\n",
    "\n",
    "    updates, new_opt_state = optim.update(grads, q_state.opt_state)\n",
    "    new_params = optax.apply_updates(q_state.params, updates)\n",
    "    q_state = q_state._replace(params=new_params, opt_state=new_opt_state)\n",
    "    buffer_state = buffer.set_priorities(buffer_state, batch.indices, new_priorities)\n",
    "\n",
    "    return loss, q_state, buffer_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, q_state, buffer_state = ger_update(q_state, buffer_state, data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax-rl-KPtyfD6I-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
